# Cursor Rules for GANGLIA Project

## Test Execution Rules

### Individual Test Execution (Root Tendril)
For individual test functions, use python command in agent window:
```python
{
  "command": "python -m pytest tests/path/to/test_file.py::test_function_name -v -s",
  "explanation": "Running individual test function",
  "is_background": false,  # Keep agent in foreground
  "require_user_approval": false
}
```

### Test Module Execution (Root System)
For complete test files, use python command in agent window:
```python
{
  "command": "python -m pytest tests/path/to/test_file.py -v -s",
  "explanation": "Running test module",
  "is_background": false,  # Keep agent in foreground
  "require_user_approval": false
}
```

### Full Test Suite Execution (Trunk)
For running full test suites:
1. First ALWAYS use run_tests.sh wrapper to start test in Composer window:
```python
{
  "command": "nohup ./run_tests.sh local unit > /dev/null 2>&1 &",
  "explanation": "Running full test suite in Composer window",
  "is_background": false,  # Keep agent in foreground
  "require_user_approval": false
}
```
2. Then IMMEDIATELY start log monitoring:
```python
{
  "command": "./utils/monitor_tests.sh",
  "explanation": "Monitoring test output",
  "is_background": false,  # Keep log monitoring in foreground
  "require_user_approval": false
}
```

### Smoke Test Execution (Branches)
For running smoke tests:
1. First start test in Composer window:
```python
{
  "command": "nohup ./run_tests.sh docker smoke > /dev/null 2>&1 &",
  "explanation": "Running smoke tests in Composer window",
  "is_background": false,  # Keep agent in foreground
  "require_user_approval": false
}
```
2. Then IMMEDIATELY start log monitoring:
```python
{
  "command": "./utils/monitor_tests.sh",
  "explanation": "Monitoring test output",
  "is_background": false,  # Keep log monitoring in foreground
  "require_user_approval": false
}
```

### Integration Test Execution (Canopy)
For running integration tests:
1. First start test in Composer window:
```python
{
  "command": "nohup ./run_tests.sh docker integration > /dev/null 2>&1 &",
  "explanation": "Running integration tests in Composer window",
  "is_background": false,  # Keep agent in foreground
  "require_user_approval": false
}
```
2. Then IMMEDIATELY start log monitoring:
```python
{
  "command": "./utils/monitor_tests.sh",
  "explanation": "Monitoring test output",
  "is_background": false,  # Keep log monitoring in foreground
  "require_user_approval": false
}
```

### Test Climbing Process
When fixing issues or making changes:

1. Start at Root Tendril (Individual Test)
   - Run specific failing test
   - Fix and verify
   - MUST proceed to Root System when passing

2. Move to Root System (Test Module)
   - Run entire test file
   - Fix any new failures by going back to Root Tendril
   - MUST proceed to Trunk when passing

3. Move to Trunk (Full Unit Suite)
   - Run in Composer window with log monitoring
   - If failures, identify failing test and go back to Root Tendril
   - MUST proceed to Branches when passing

4. Move to Branches (Smoke Tests)
   - Run in Composer window with log monitoring
   - If failures, trace back to appropriate Root System
   - Consider moving to Canopy if changes warrant

5. Move to Canopy (Integration Tests)
   - Run if changes affect multiple systems
   - Run in Composer window with log monitoring
   - If failures, trace back to appropriate level

### Critical Rules
1. NEVER skip levels when climbing up
2. ALWAYS run the next level up after a fix
3. When descending to debug, start at the failing test
4. Document any decisions to stop climbing before Canopy
5. NEVER interrupt tests unless they fail
6. ALWAYS use Composer window for Trunk level and above
7. ALWAYS wait for completion signal before proceeding
8. ALWAYS monitor test output in real-time for Trunk level and above

### Log Monitoring Rules
1. NEVER kill the log monitoring command (tail -f)
   - Let it run until test completion
   - Expect periods of no output (up to 1 minute)
   - Only stop if test fails and debugging is needed

2. ALWAYS check the most recent test run logs
   - Look at timestamp in log filename
   - Most recent timestamp = current test run
   - Older timestamps = previous test runs

3. Monitor logs in real-time
   - Keep log window visible
   - Watch for test progress
   - Look for error messages
   - Note any warnings

4. Log files contain critical information
   - Test output and errors
   - Debug information
   - Progress indicators
   - Final results

### Status File Handling
Each test level above Root System uses timestamped status files:

1. Trunk (Unit Tests):
   - Status file: `/tmp/GANGLIA/test_status_TIMESTAMP.txt`
   - Cleaned up automatically before each run

2. Branches (Smoke Tests):
   - Status file: `/tmp/GANGLIA/smoke_status_TIMESTAMP.txt`
   - Cleaned up automatically before each run

3. Canopy (Integration Tests):
   - Status file: `/tmp/GANGLIA/integration_status_TIMESTAMP.txt`
   - Cleaned up automatically before each run

### Log Files
All test runs above Root System level generate timestamped log files:
- Location: `logs/test_run_MODE_TYPE_TIMESTAMP.log`
- Contains full test output and debug information
- Preserved for debugging and analysis
- MUST be monitored in real-time during test execution

## Issue Reporting Protocol
When reporting an issue, follow these steps:

### Information Gathering
Required information to be collected in a single interaction:
1. Issue Type:
   - bug: A problem with existing functionality
   - enhancement: A new feature or improvement
   - documentation: Documentation-related issues
   - test: Test-related issues
   - ci: CI/CD pipeline issues

2. Title: A clear, concise title that summarizes the issue

3. Description Template:
```markdown
### Current Behavior
[What is happening now]

### Expected Behavior
[What should happen instead]

### Steps to Reproduce (if applicable)
1. [First Step]
2. [Second Step]
3. [...]

### Additional Context
- Environment: [e.g., local/CI, OS, relevant versions]
- Related Components: [e.g., TTV, Tests, Music Generation]
- Impact Level: [low/medium/high]
```

### Issue Creation Process
1. Present the formatted issue using the template above
2. After approval, create the issue using gh cli:
```bash
gh issue create \
  --title "TITLE" \
  --body "BODY" \
  --label "TYPE"
```
3. Display the created issue URL

### Example Usage
For the credits cutoff issue:
```bash
gh issue create \
  --title "Video credits abruptly cut off at 30 seconds in integration tests" \
  --body "### Current Behavior
Credits section in generated videos is being cut off at exactly 30 seconds during integration tests.

### Expected Behavior
Credits should play completely without being cut off.

### Steps to Reproduce
1. Run integration tests
2. Check generated video output
3. Observe credits section ending abruptly at 30s mark

### Additional Context
- Environment: CI pipeline
- Related Components: TTV, Integration Tests
- Impact Level: medium" \
  --label "bug"
```

## Decision Making and Proactivity Rules
1. Be proactive - take your best guess and proceed rather than asking for confirmation
2. When encountering a decision point:
   - Make a reasoned choice based on available context
   - Document the choice and rationale in your response
   - Proceed with implementation
   - Let the user course-correct if needed
3. Avoid blocking on user input unless absolutely necessary
4. If multiple reasonable approaches exist:
   - Choose the most likely one
   - Note alternatives in your response
   - Continue with implementation
5. Bias towards action over asking questions

## AI Status Tracking Rules
You MUST maintain the .ai_status.md file to track task progress:

1. At the start of each new task:
   - Update the "Current Task" section with a clear description
   - Include timestamp in PST
   - Format: `Started: [task description] (MM/DD/YYYY, HH:MM:SS AM/PM PST)`

2. When completing a task:
   - Move the task from "Current Task" to "Task History" section
   - Include completion timestamp
   - Format: `- Completed: [task description] (MM/DD/YYYY, HH:MM:SS AM/PM PST)`
   - Clear the "Current Task" section to "No active task"

3. If a task is abandoned or fails:
   - Move the task to "Task History" with appropriate status
   - Format: `- [Abandoned/Failed]: [task description] (MM/DD/YYYY, HH:MM:SS AM/PM PST)`
   - Include brief reason for abandonment/failure
   - Clear the "Current Task" section to "No active task"

4. For multi-step tasks:
   - Keep main task in "Current Task"
   - Add progress updates as bullet points under current task
   - Format: `- [step description] (MM/DD/YYYY, HH:MM:SS AM/PM PST)`

5. Status file structure must be maintained:
```markdown
# AI Agent Status

## Current Task
[Current task description or "No active task"]
[Optional progress updates for multi-step tasks]

## Task History
[Chronological list of completed/abandoned tasks with timestamps]
