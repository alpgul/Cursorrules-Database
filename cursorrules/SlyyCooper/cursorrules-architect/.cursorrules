<system_prompt>
You are an intelligent senior software developer and AI code generator responsible for the project shown in the the XML tags.

Your Requirements:
- Maintain a clean, organized and modular codebase by seperating code out code logically into appropriate files, directories, sub-directories.
- Do not let any file exceed ~250 lines of code.
- Always use the latest libraries and tools as of 2025.
- Take a modular approach by creating new files where needed, sectioning out code into logical files.
- When encountering syntax errors or bugs, carefully examine the full file content before making changes
- Leave begineer friendly comments logically sectioning out each file with headers describing what each section does.
</system_prompt>

## DeepSeek API Notes

The DeepSeek API uses an API format compatible with OpenAI. By modifying the configuration, you can use the OpenAI SDK or software compatible with the OpenAI API to access the DeepSeek API.

### Configuration Parameters:

| PARAM      | VALUE                  |
|------------|------------------------|
| `base_url` * | `https://api.deepseek.com` |
| `api_key`  | Apply for an API key   |

*   **Compatibility Note:** You can also use `https://api.deepseek.com/v1` as the `base_url` for OpenAI compatibility. Note that the `v1` here has NO relationship with the model's version.

### Model Information:

*   **`deepseek-chat`**: This model has been upgraded to **DeepSeek-V3**. The API remains unchanged. Invoke DeepSeek-V3 by specifying `model='deepseek-chat'`.
*   **`deepseek-reasoner`**: This is the latest reasoning model, **DeepSeek-R1**, released by DeepSeek. Invoke DeepSeek-R1 by specifying `model='deepseek-reasoner'`.

## Google Gemini API Notes

### Text Generation

The Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.

**NOTE:** Use the New Gemini API 'from google import genai'

#### Text Input

The simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=["How does AI work?"]
)
print(response.text)
```

#### Streaming Output

By default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of `GenerateContentResponse` as they're generated.

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents=["Explain how AI works"]
)
for chunk in response:
    print(chunk.text, end="")
```

#### Multi-turn Conversations

The Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same `generateContent` method to create the response.

The following code example shows a basic chat implementation:

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")
chat = client.chats.create(model="gemini-2.0-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

# Print chat history
for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)
```

You can also use streaming with chat, as shown in the following example:

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")
chat = client.chats.create(model="gemini-2.0-flash")

response = chat.send_message_stream("I have 2 dogs in my house.")
for chunk in response:
    print(chunk.text, end="")
print() # Add a newline after the first streamed response

response = chat.send_message_stream("How many paws are in my house?")
for chunk in response:
    print(chunk.text, end="")
print() # Add a newline after the second streamed response

# Print chat history
for message in chat.get_history():
    print(f'role - {message.role}', end=": ")
    print(message.parts[0].text)
```

#### System Instructions

System instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.

You can set system instructions when you initialize your model:

```python
from google import genai
from google.genai import types

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    config=types.GenerateContentConfig(
        system_instruction="You are a cat. Your name is Neko."),
    contents="Hello there"
)

print(response.text)
```

### Use Gemini Thinking

Gemini 2.5 Pro Experimental and Gemini 2.0 Flash Thinking Experimental are models that use an internal "thinking process" during response generation. This process contributes to their improved reasoning capabilities and allows them to solve complex tasks. This guide shows you how to use Gemini models with thinking capabilities.

*Try Gemini 2.5 Pro Experimental in Google AI Studio*

#### Use Thinking Models

Models with thinking capabilities are available in Google AI Studio and through the Gemini API. Note that the thinking process is visible within Google AI Studio but is not provided as part of the API output.

##### Send a Basic Request

```python
from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")
prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."
response = client.models.generate_content(
    model="gemini-2.5-pro-exp-03-25",  # or gemini-2.0-flash-thinking-exp
    contents=prompt
)

print(response.text)
```

##### Multi-turn Thinking Conversations

To take the previous chat history into account, you can use multi-turn conversations.

With the SDKs, you can create a chat session to manage the state of the conversation. *Note: The example below uses `asyncio`.*

```python
import asyncio
from google import genai

async def run_async_chat():
    client = genai.Client(api_key='GEMINI_API_KEY')

    chat = client.aio.chats.create(
        model='gemini-2.5-pro-exp-03-25',  # or gemini-2.0-flash-thinking-exp
    )
    response = await chat.send_message('What is your name?')
    print(response.text)
    response = await chat.send_message('What did you just say before this?')
    print(response.text)

# To run the async function:
# asyncio.run(run_async_chat())
```

<project_structure>
.
├── .env
├── CONTRIBUTING.md
├── README.md
├── config
│   ├── __init__.py
│   ├── agents.py
│   ├── exclusions.py
│   ├── prompts
│   │   ├── __init__.py
│   │   ├── final_analysis_prompt.py
│   │   ├── phase_1_prompts.py
│   │   ├── phase_2_prompts.py
│   │   ├── phase_3_prompts.py
│   │   ├── phase_4_prompts.py
│   │   ├── phase_5_prompts.py
├── core
│   ├── __init__.py
│   ├── agents
│   │   ├── __init__.py
│   │   ├── anthropic.py
│   │   ├── base.py
│   │   ├── deepseek.py
│   │   ├── gemini.py
│   │   ├── openai.py
│   ├── analysis
│   │   ├── __init__.py
│   │   ├── final_analysis.py
│   │   ├── phase_1.py
│   │   ├── phase_2.py
│   │   ├── phase_3.py
│   │   ├── phase_4.py
│   │   ├── phase_5.py
│   ├── types
│   │   ├── __init__.py
│   │   ├── agent_config.py
│   ├── utils
│   │   ├── file_creation
│   │   │   ├── cursorignore.py
│   │   │   ├── phases_output.py
│   │   ├── tools
│   │   │   ├── agent_parser.py
│   │   │   ├── file_retriever.py
│   │   │   ├── model_config_helper.py
│   │   │   ├── tree_generator.py
├── main.py
├── requirements.txt
├── tests
│   ├── final_analysis_test
│   │   ├── fa_test_input.json
│   │   ├── output
│   │   │   ├── cursor_rules.md
│   │   │   ├── final_analysis_results.json
│   │   ├── run_test.py
│   │   ├── test_date.py
│   ├── phase_1_test
│   │   ├── output
│   │   │   ├── phase1_results.json
│   │   ├── run_test.py
│   ├── phase_2_test
│   │   ├── output
│   │   │   ├── analysis_plan.xml
│   │   │   ├── phase2_results.json
│   │   ├── run_test.py
│   │   ├── test2_input.json
│   ├── phase_3_test
│   │   ├── debug_parser.py
│   │   ├── output
│   │   │   ├── phase3_results.json
│   │   ├── run_test.py
│   │   ├── test3_input.json
│   │   ├── test3_input.xml
│   ├── phase_4_test
│   │   ├── output
│   │   │   ├── analysis.md
│   │   │   ├── phase4_results.json
│   │   ├── run_test.py
│   │   ├── test4_input.json
│   ├── phase_5_test
│   │   ├── output
│   │   │   ├── consolidated_report.md
│   │   │   ├── phase5_results.json
│   │   ├── run_test.py
│   │   ├── test5_input.json
│   ├── tests_input
│   │   ├── README.md
│   │   ├── index.html
│   │   ├── main.py
│   │   ├── requirements.txt
│   ├── utils
│   │   ├── run_tree_generator.py
</project_structure>